{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration: IMDb Movie Reviews Dataset\n",
        "\n",
        "## Overview\n",
        "\n",
        "The IMDb dataset contains 50,000 movie reviews labeled as positive (1) or negative (0). This is a classic binary sentiment classification task that allows us to compare classical ML and deep learning approaches.\n",
        "\n",
        "## Dataset Characteristics\n",
        "\n",
        "- **Source**: Stanford AI Lab\n",
        "- **Task**: Binary sentiment classification\n",
        "- **Samples**: 50,000 reviews (25,000 train, 25,000 test)\n",
        "- **Classes**: Positive (1) and Negative (0)\n",
        "- **Text Length**: Variable (typically 100-2000 words per review)\n",
        "\n",
        "## Why This Dataset?\n",
        "\n",
        "1. **Standard Benchmark**: Widely used in NLP research\n",
        "2. **Clear Task**: Binary classification is interpretable\n",
        "3. **Real-world Relevance**: Sentiment analysis has practical applications\n",
        "4. **Text Complexity**: Reviews contain varied vocabulary and style\n",
        "5. **Size**: Large enough to demonstrate scalability differences between approaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load IMDb dataset\n",
        "# num_words: keep top 10,000 most frequent words\n",
        "# skip_top: skip most frequent words (often stopwords)\n",
        "# maxlen: maximum sequence length (None = no limit)\n",
        "print(\"Loading IMDb dataset...\")\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000, skip_top=0, maxlen=None)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Test labels shape: {y_test.shape}\")\n",
        "print(f\"\\nLabel distribution (training):\")\n",
        "print(f\"  Positive (1): {np.sum(y_train == 1)}\")\n",
        "print(f\"  Negative (0): {np.sum(y_train == 0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get word index mapping\n",
        "word_index = imdb.get_word_index()\n",
        "# Reverse mapping: word_index -> word\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "# Decode a review to see what it looks like\n",
        "def decode_review(encoded_review):\n",
        "    \"\"\"Decode an encoded review back to text.\"\"\"\n",
        "    # Word indices are offset by 3 (0: padding, 1: start, 2: unknown)\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
        "\n",
        "# Show example reviews\n",
        "print(\"Example Positive Review (first 500 chars):\")\n",
        "print(decode_review(X_train[0])[:500])\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Example Negative Review (first 500 chars):\")\n",
        "print(decode_review(X_train[np.where(y_train == 0)[0][0]])[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze review lengths\n",
        "train_lengths = [len(review) for review in X_train]\n",
        "test_lengths = [len(review) for review in X_test]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(train_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Review Length (words)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Review Lengths (Training)')\n",
        "axes[0].axvline(np.mean(train_lengths), color='red', linestyle='--', \n",
        "               label=f'Mean: {np.mean(train_lengths):.1f}')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].boxplot([train_lengths, test_lengths], labels=['Train', 'Test'])\n",
        "axes[1].set_ylabel('Review Length (words)')\n",
        "axes[1].set_title('Review Length Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training set statistics:\")\n",
        "print(f\"  Mean length: {np.mean(train_lengths):.1f} words\")\n",
        "print(f\"  Median length: {np.median(train_lengths):.1f} words\")\n",
        "print(f\"  Min length: {np.min(train_lengths)} words\")\n",
        "print(f\"  Max length: {np.max(train_lengths)} words\")\n",
        "print(f\"\\nTest set statistics:\")\n",
        "print(f\"  Mean length: {np.mean(test_lengths):.1f} words\")\n",
        "print(f\"  Median length: {np.median(test_lengths):.1f} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to text format for classical ML approach\n",
        "# We'll save both encoded and text versions\n",
        "def convert_to_text(encoded_reviews):\n",
        "    \"\"\"Convert encoded reviews to text strings.\"\"\"\n",
        "    texts = []\n",
        "    for review in encoded_reviews:\n",
        "        texts.append(decode_review(review))\n",
        "    return texts\n",
        "\n",
        "# For classical ML, we'll use a subset for faster processing\n",
        "# Full dataset can be used, but we'll use 10,000 samples for demonstration\n",
        "n_samples_classical = 10000\n",
        "\n",
        "# Sample indices\n",
        "train_indices = np.random.choice(len(X_train), n_samples_classical, replace=False)\n",
        "test_indices = np.random.choice(len(X_test), min(n_samples_classical, len(X_test)), replace=False)\n",
        "\n",
        "# Convert to text\n",
        "X_train_text = [decode_review(X_train[i]) for i in train_indices]\n",
        "X_test_text = [decode_review(X_test[i]) for i in test_indices]\n",
        "y_train_classical = y_train[train_indices]\n",
        "y_test_classical = y_test[test_indices]\n",
        "\n",
        "print(f\"Classical ML dataset:\")\n",
        "print(f\"  Training: {len(X_train_text)} samples\")\n",
        "print(f\"  Test: {len(X_test_text)} samples\")\n",
        "\n",
        "# Save for use in other notebooks\n",
        "import pickle\n",
        "\n",
        "data_dict = {\n",
        "    'X_train_text': X_train_text,\n",
        "    'X_test_text': X_test_text,\n",
        "    'y_train_classical': y_train_classical,\n",
        "    'y_test_classical': y_test_classical,\n",
        "    'X_train_encoded': X_train,\n",
        "    'X_test_encoded': X_test,\n",
        "    'y_train': y_train,\n",
        "    'y_test': y_test,\n",
        "    'word_index': word_index\n",
        "}\n",
        "\n",
        "with open('../imdb_data.pkl', 'wb') as f:\n",
        "    pickle.dump(data_dict, f)\n",
        "\n",
        "print(\"\\nData saved to '../imdb_data.pkl'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
