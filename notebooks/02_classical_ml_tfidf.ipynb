{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classical ML Approach: TF-IDF + Logistic Regression\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a classical machine learning pipeline for sentiment classification:\n",
        "1. **Text Preprocessing**: Cleaning and normalization\n",
        "2. **TF-IDF Vectorization**: Convert text to numerical features\n",
        "3. **Logistic Regression**: Train classifier on TF-IDF features\n",
        "\n",
        "## Why TF-IDF?\n",
        "\n",
        "- **Interpretability**: Features correspond to actual words\n",
        "- **Efficiency**: Fast training and prediction\n",
        "- **Works well with limited data**: Doesn't require large datasets\n",
        "- **No GPU needed**: Runs efficiently on CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import re\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "with open('../imdb_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train_text = data['X_train_text']\n",
        "X_test_text = data['X_test_text']\n",
        "y_train = data['y_train_classical']\n",
        "y_test = data['y_test_classical']\n",
        "\n",
        "print(f\"Training samples: {len(X_train_text)}\")\n",
        "print(f\"Test samples: {len(X_test_text)}\")\n",
        "print(f\"\\nClass distribution (training):\")\n",
        "print(f\"  Positive: {np.sum(y_train == 1)}\")\n",
        "print(f\"  Negative: {np.sum(y_train == 0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "We'll clean the text by:\n",
        "- Converting to lowercase\n",
        "- Removing special characters\n",
        "- Removing stopwords (optional, can be controlled in TF-IDF)\n",
        "- Basic normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits (keep only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Preprocess texts\n",
        "print(\"Preprocessing texts...\")\n",
        "X_train_processed = [preprocess_text(text) for text in X_train_text]\n",
        "X_test_processed = [preprocess_text(text) for text in X_test_text]\n",
        "\n",
        "print(\"Example original text:\")\n",
        "print(X_train_text[0][:200])\n",
        "print(\"\\nExample processed text:\")\n",
        "print(X_train_processed[0][:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Vectorization\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) converts text documents into numerical feature vectors where:\n",
        "- **TF**: How frequently a word appears in a document\n",
        "- **IDF**: How rare a word is across all documents\n",
        "\n",
        "This gives higher weights to words that are distinctive to a document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create TF-IDF vectorizer\n",
        "# max_features: limit vocabulary size for efficiency\n",
        "# ngram_range: use unigrams and bigrams\n",
        "# min_df: ignore terms that appear in less than 2 documents\n",
        "# max_df: ignore terms that appear in more than 95% of documents (likely stopwords)\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),  # unigrams and bigrams\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "print(\"Fitting TF-IDF vectorizer...\")\n",
        "start_time = time.time()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
        "tfidf_fit_time = time.time() - start_time\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
        "\n",
        "print(f\"TF-IDF transformation complete in {tfidf_fit_time:.2f} seconds\")\n",
        "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Test features shape: {X_test_tfidf.shape}\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Logistic Regression Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "start_time = time.time()\n",
        "lr_classifier = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    C=1.0  # Inverse of regularization strength\n",
        ")\n",
        "lr_classifier.fit(X_train_tfidf, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training complete in {training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = lr_classifier.predict(X_test_tfidf)\n",
        "y_pred_proba = lr_classifier.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - TF-IDF + Logistic Regression')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "One advantage of TF-IDF + Logistic Regression is interpretability. We can see which words are most important for positive/negative classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names and coefficients\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "coefficients = lr_classifier.coef_[0]\n",
        "\n",
        "# Get top positive and negative features\n",
        "top_n = 20\n",
        "top_positive_indices = np.argsort(coefficients)[-top_n:][::-1]\n",
        "top_negative_indices = np.argsort(coefficients)[:top_n]\n",
        "\n",
        "print(\"Top 20 Features for Positive Sentiment:\")\n",
        "for idx in top_positive_indices:\n",
        "    print(f\"  {feature_names[idx]:20s} : {coefficients[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 20 Features for Negative Sentiment:\")\n",
        "for idx in top_negative_indices:\n",
        "    print(f\"  {feature_names[idx]:20s} : {coefficients[idx]:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "top_pos_words = [feature_names[idx] for idx in top_positive_indices]\n",
        "top_pos_coefs = [coefficients[idx] for idx in top_positive_indices]\n",
        "\n",
        "top_neg_words = [feature_names[idx] for idx in top_negative_indices]\n",
        "top_neg_coefs = [coefficients[idx] for idx in top_negative_indices]\n",
        "\n",
        "axes[0].barh(range(len(top_pos_words)), top_pos_coefs)\n",
        "axes[0].set_yticks(range(len(top_pos_words)))\n",
        "axes[0].set_yticklabels(top_pos_words)\n",
        "axes[0].set_xlabel('Coefficient')\n",
        "axes[0].set_title('Top Features for Positive Sentiment')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "axes[1].barh(range(len(top_neg_words)), top_neg_coefs)\n",
        "axes[1].set_yticks(range(len(top_neg_words)))\n",
        "axes[1].set_yticklabels(top_neg_words)\n",
        "axes[1].set_xlabel('Coefficient')\n",
        "axes[1].set_title('Top Features for Negative Sentiment')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results for Comparison\n",
        "\n",
        "We'll save the results to compare with the deep learning approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "results_classical = {\n",
        "    'accuracy': accuracy,\n",
        "    'training_time': training_time,\n",
        "    'tfidf_fit_time': tfidf_fit_time,\n",
        "    'total_time': training_time + tfidf_fit_time,\n",
        "    'model': lr_classifier,\n",
        "    'vectorizer': tfidf_vectorizer\n",
        "}\n",
        "\n",
        "with open('../results_classical.pkl', 'wb') as f:\n",
        "    pickle.dump(results_classical, f)\n",
        "\n",
        "print(\"Results saved to '../results_classical.pkl'\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"  Total Time (including TF-IDF): {results_classical['total_time']:.2f} seconds\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
